---
title: "DDPG & TD3"
excerpt: "DDPG & TD3"
date: 2022-06-09 00:00:00 +0900
header:
  image: /assets/images/unsplash-emile-perron.jpg
  caption: "Photo by [**Emile Perron**](https://unsplash.com/@emilep) on [**Unsplash**](https://unsplash.com/)"
categories:
  - Python
mathjax: "true"
---

# Deep Deterministic Policy Gradient(DDPG)

원문: https://spinningup.openai.com/en/latest/algorithms/ddpg.html

## Background

Deep Deterministic Policy Gradient(DDPG)는 Q-function과 policy를 동시에 학습하는 알고리즘이다.
Q-function을 학습하기 위해 off-policy data와 Bellman equation을 사용하고, policy를 학습하기 위해 Q-function을 사용한다.

이 접근법은 Q-learning과 밀접하게 관련되어 있으며, 다음의 동일한 모티브를 사용한다:
만약 optimal action-value function $Q^\*(s, a)$를 알고 있다면, 어떤 주어진 state s에서 optimal action $a^\*(s)$는 다음을 통해 얻을 수 있다.

$$a^*(s) = \arg \max_a Q^*(s, a)$$

DDPG는 $Q^\*(s, a)$에 대한 approximator를 배우는 것과 $a^\*(s)$에 대한 approximator를 배운느 것을 interleave한다.
그리고 특히 continuouse한 action space를 가진 environmet들에 적용된다.

하지만 DDPG가 *특별히* 그러한 continuous actions space들을 가진 environment들에만 적용된다는 것은 무엇을 의미할까?
이는 우리가 action들에 대해 max를 계산하는 방법 $\max_a Q^\*(s, a)$과 관련이 있다.

한정된 숫자의 discrete action이 있을 때, max poses는 문제가 안된다. 왜냐하면 우리는 각 액션에 대해 각각 Q-value를 계산할 수 있으며, 이를 직접 서로 비교할 수 있다.
(동시에 Q-value를 최대화하는 액션도 알 수 있다)
하지만 action space가 continuous하다면, 각 공간을 일일이 평가할 수 없으며, optimization problem을 푸는 것은 쉽지 않습니다.
일반적인 최적화 알고리즘을 사용하여 $\max_a Q^\*(s, a)$를 계산해내는 것은 매우 expensive한 subroutine입니다. 
그리고 agent가 액션을 고를 때 매번 이런 과정이 필요하므로 이는 불가능합니다.

action space가 continuous하기 때문에 $Q^\*(s, a)$는 action argument에 대해 differentiable(미분가능한)이라고 여겨집니다.
이로부터 효율적인 policy $\mu(s)$를 만들어 내는 gradient-based learning rule을 set up할 수 있습니다.
그러면 매번 $Q^\*(s, a)$를 계산할 때마다 expensive optimization subroutine을 수행할 필요가 없어지며, 우리는 $Q^\*(s, a) \sim Q(s, \mu(s))$를 통해 approximate를 할 수 있습니다.
아래의 Key Equation 섹션을 참고하세요.

## Quick Facts
- DDPG는 off-policy algorithm이다.
- DDPG는 continuous action space를 가진 environment에만 사용할 수 있다.
- DDPG는 continuous action space를 위한 deep Q-learning으로 생각될 수 있다.
- Spinning Up의 DDPG 구현은 parallelization은 지원하지 않는다.


## Key Equations

여기에선 DDPG의 두 가지 수학 배경을 설명한다: Q function 학습, Policy 학습

## The Q-Learning Side of DDPG

먼저 optimal action-value function $Q^\*(s, a)$를 설명하는 Bellman equation을 복습하자.

$$Q^*(s, a) = E_{s' \sim P}[r(s, a) + \gamma max_{a'}Q^*(s', a')]$$

여기서 $s' \sim P$는 next state $s'$가 environment로부터 $P(\cdot \| s, a)$ 분포로 샘플된다는 뜻이다.

Bellman Equation은 $Q^\*(s, a)$에 대한 approximator를 학습하는 데에서 시작한다.
그 approximator를 파라미터 $\phi$를 가진 neural network $Q_{\phi}(s, a)$라고 하고, Transition $(s, a, r, s', d)$ ($d$는 state $s'$가 terminal인지 여부를 나타낸다)의 집합 $D$를 모았다고 하자.
그러면 mean-squared Bellman error(MSBE) 함수를 정의할 수 있다. 이는 $Q_{\phi}$가 얼마나 Bellman equation에 근접하는지 나타낸다.

$$L(\phi, D) = E_{(s,a,r,s',d) \sim D} [(Q_{\phi}(s, a) - (r + \gamma(1-d) \max_{a'} Q_{\phi}(s', a')))^2]$$

여기에서 $(1-d)$를 계산할 때, `True`를 1로 취급하고 `False`를 0으로 취급하는 python convention을 사용했다.
`d==True`일 때, 즉 $s'$가 terminal state일 때 Q-function은 agent가 현재 state 이후로 추가적인 reward를 받지 않는 다는 것을 뜻한다.
(이 notation은 추후 구현 코드에서도 동일하다)

function approximator를 위한 Q-learing 알고리즘들(DQN 변형들과 DDPG)은 MSBE loss functiond을 최소화하는데 기반을 둔다.
두 개의 설정할 가치가 있는 두 main trick을 설명하고, DDPG의 세부 디테일을 설명하겠다.

### Trick One: Replay Buffers

$Q^\*(s, a)$를 approximate하는 deep neural network를 훈련하는 모든 표준 알고리즘들은 experient replay buffer를 활용한다. 
이는 이전의 경ㅎ험들로 구성된 세트 $D$이다. 알고리즘이 안정적인 행동을 하도록하기 위해 replay buffer는 많은 범위의 경험을 저장할 수 있도록 충분히 커야한다.
하지만 항상 모든 것을 저장하는 것이 좋은 것은 아니다.
만약 가장 최근의 데이터만 사용한다면 그것에 맞춰 오버핏이 될 것이다.
만약 너무 많은 경험을 사용한다면, 학습이 느려질 수 있다.
적당한 설정을 위해 튜닝이 필요할 수 있다.

> 알아야할 것
> 앞서 DDPG가 off-policy algorithm이라고 언급했다. 이쯤에서 그 이유와 방법을 설정하기 좋은 시점인것 같다.
> replay buffer는 이전의 경험들을 포함해야한다. 
> (이는 outdated policy로부터 생성된 것임에도 불구하고)
> 왜 이를 다 사용할 수 있을까?
> 그 이유는 Bellman equation은 어떤 transition tuple이 사용되었는지, 어떻게 액션이 선택되었는지, 이 transition 이후에 어떤 일이 벌어졌는지 신경쓰지 않기 때문이다.
> 왜냐하면 optimal Q-function는 가능한 *모든* transition에 대해 Bellman eqation이 만족되어야 하기 때문이다.
> 그래서 모든 transition을 MSBE minimization을 통한 Q-function approximator fit에 사용할 수 있다.


### Trick Two: Target Networks

Q-learning algorithmd은 target network를 활용한다.

$$r + \gamma (1 - d) \max_{a'} Q_{\phi}(s', a')$$

를 target이라고 부른다. 
왜냐하면 MSBE loss를 감소시킬 때 Q-function을 위 target에 가깝게 만들려고 하기 때문이다.

target은 우리가 훈련하려는 것과 같은 파라미터 $\phi$에 의존한다. 

이는 MSBE minimization을 unstable하게 만든다.

해법은 $\phi$와 가까운 파라미터를 사용하되 딜레이가 있는 세트를 사용하는 것이다.
즉, 타겟 네트워크라고 불리는 두 번째 네트워크를 사용한다.
타겟 네트워크의 파라미터는 $\phi_{targ}$로 표기된다.

DQN 기반 알고리즘들에서, target network는 특정 고정된 step마다 main network로부터 복사된다.

DDPG-style 알고리즘에서는, polyak averaging을 통해 main network가 update될 때마다 target network도 업데이트 된다.

$$\phi_{targ} = \rho \phi_{targ} + (1 - \rho)\phi$$

$\rho$는 0에서 1 사이 값을 가지는 hyperparameter이며, 주로 1에 가깝게 사용한다.
(이 하이퍼 파라미터는 코드에서 `polyak`으로 불린다)

### DDPG Detail: Calculating the Max Over Actions in the Target

앞에서 언급했듯이: continuous space action 중 maximum을 계산하는 것은 어렵다.
DDPG는 **target policy network**를 통해 $Q_{\phi_{targ}}$를 최대화하는 action을 계산해내는 방식을 통해 이 문제를 해결한다.
target policy network는 target Q-function과 동일한 방식이다: 훈련 중 policy parameter를 polyak averaging을 한다.

이를 종합하면, DDPG에서 Q-learning은 다음의 MSBE loss를 minimizing하는 stochastic gradient descent를 통해 수행된다.

$$L(\phi, D) = E_{(s,a,r,s',d) \sim D} [(Q_\phi(s, a) - (r + \gamma(1 - d)Q_{\phi_{targ}}(s', \mu_{\theta_{targ}}(s'))))^2]$$

여기서 $\mu_{\theta_{targ}}$는 target policy이다.


### The Policy Learning Side of DDPG

DDPG의 Policy learning은 상당히 간단하다. 우리는 deterministic policy $\mu_{\theta}(s)$를 찾고 싶다. 이는 $Q_{\phi}(s, a)$를 maximize하는 액션을 선정한다.
action space가 continuous하고, 우리는 Q-function이 action에 대해 differentiable하다고 가정했기 떄문에, 우리는 단순히 다음을 푸는 gradient ascent(policy network에 대해서만)를 수행하면 된다.

$$\max_{\theta} E_{s \sim D} [Q_{\phi}(s, \mu_{\theta}(s)]$$

Q-function의 파라미터는 여기서 constant로 취급된 것에 주목하자.

### Exploration vs. Exploitation

DDPG는 deterministic policy를 off-policy 방식으로 학습한다. policy가 deterministic하기 떄문에 on-policy로 explore할 때 학습에 유용한 액션을 찾기 위한 액션들을 시도하지 않을 수 있다.
DDPG가 explore를 하게 하기 위해서 training 시에는 noise를 추가한다. DDPG 논문의 저자는 time-correlated OU noise를 추천하였으나, 최근 결과들에서는 uncorrelated mean-zero Gaussian noise도 잘 동작함을 확인하였다.
후자가 더 간단하기 때문에 선호된다.
Higher-quality의 training data를 가능케하기 위해서, training 과정 중에 noise의 크기를 줄일 수 있다.
(우리는 구현에서 이를 적용하지 않았으며, 정해진 노이즈 크기를 유지한다.)

test할 때에는 policy가 학습으로부터 잘 exploits를 하는 것을 확인하기 위해 액션에 노이즈를 추가하지 않았다.

> 알아야 할 것
> 우리의 DDPG 구현은 학습 초기에 exploration을 향상시키기 위해 트릭을 사용하였다.
> 초기의 특정한 수의 스텝에 대해(`start_steps` argument로 설정),
> agent는 uniform random distribution으로부터 액션을 샘플링한다.
> 그리고나서 일반적인 DDPG exploration으로 돌아간다.

### Pseudocode

```python
# Algorithm 1: Deep Deterministic Policy Gradient
#
# Input: initial policy parameters θ, Q-function parameters φ, empty replay buffer D
# Set target parameters equal to main parameters θ_targ <- θ, φ_targ <- φ

while not done:
  # Observe state s and select action a = clip(μ_θ(s) + ε, a_low, a_high), where ε ~ N
  # Execute a in the environment
  # Observe next state s', reward r, and done signal d to indicate whether s' is terminal
  # Store (s, a, r, s', d) in replay buffer D
  # If s' is terminal, reset environment state.
  if time_to_update:
    for i in many_updates:
      # Randomly sample a batch of transitions, B = {(s, a, r, s', d)} from D
      # Compute target

      # y(r, s', d) = r + γ(1-d) Qφ_targ(s', μθ_targ(s'))

      # Update Q-function by one step of gradient descent using

      # (See Equation in spinning up webpage: https://spinningup.openai.com/en/latest/algorithms/ddpg.html)

      # Update policy by one step of gradient ascent using

      # (See Equation in spinning up webpage: https://spinningup.openai.com/en/latest/algorithms/ddpg.html)

      # Update target networks

      # φ_targ <- ρ φ_targ + (1 - ρ) φ
      # θ_targ <- ρ θ_targ + (1 - ρ) θ
```


# Twin Delayed DDPG

원문: https://spinningup.openai.com/en/latest/algorithms/td3.html

DDPG가 때때로 훌륭한 성능을 보이지만, hypterparameter나 다른 tuning에 의해 쉽게 불안정해진다.
DDPG의 흔한 실패는 Q-function이 과도하게 Q-value를 overestimate하기 시작하는 것이다.
이는 policy breaking으로 이어진다. 
이는 Q-function의 오차를 exploit하기 때문이다. 

Twin Delayed DDPG(TD3)는 이 이슈를 세 개의 trick을 통해 접근한다.

## Trick One: Clipped Double-Q Learning

TD3는 Q-function 하나를 사용하는 대신에 두 개를 학습한다.
Bellman error loss function의 타겟을 계산할 때 두 개의 Q-value 중 작은 것을 사용한다.

## Trick Two: "Delayed" Policy Updates

TD3는 Policy(와 target networks)를 Q-function보다 덜 자주 업데이트한다.
논문에서는 두 번의 Q-function update 당 한 번의 policy update를 추천한다.

## Trick Three: Target Policy Smoothing

TD3는 target action에 노이즈를 추가한다.
그래서 Q를 action change에 따라 smoothing하며 policy가 Q-function error를 exploit하는 것을 힘들게 만든다.

종합하면, 이 세 개의 트릭은 baseline DDPG에 비해 상당한 성능 향상을 이루었다.

## Quick Facts
- TD3 is an off-policy algorithm.
- TD3 can only be used for environments with continuous action spaces.
- The Spinning Up implementation of TD3 does not support parallelization.

## Key Equations

TD3는 mean square Bellman error를 minimize하며 두 개의 Q-function($Q_{\phi 1}$, $Q_{\phi 2}$)을 동시에 학습한다.
이는 DDPG에서 하나의 Q-function을 학습한 것과 거의 유사하다.
정확히 TD3에서 이를 어떻게 하는지, 일반 DDPG와는 무엇이 다른지 보이기 위해서 loss function의 가장 안쪽 부분에서 밖으로 보이겠다.

First: **target policy smoothing**. Q-learning target을 위한 액션은 target policy, $\mu_{\theta_{targ}}$에 기반하는데, 각 액션의 차원에 대해 clipped noise를 추가된다.
clipped noise를 추가하고 나서, target action은 valid action range에 놓이게 된다($a_{Low} \le a \le a_{High}$). Target action은 다음과 같다.

$$a'(s') = clip( \mu_{\theta_{targ}}(s') + clip(\epsilon, -c, c), a_{Low}, a_{High}),\quad\epsilon \sim N(0, \sigma)$$

Target policy smoothing은 기본적으로 이 알고리즘의 regularizer로 역할한다.
이는 DDPG에서 발생하는 실패를 완화한다: Q-function approximator가 몇몇 action에 대해 incorrect sharp peak를 형성할 때, policy는 이 peak를 빠르게 exploit하고 불안정해지고 잘못된 행동을 하게 된다.
이는 Q-function을 유사한 액션들로부터 smoothing을 하여 방지될 수 있으며, 이는 target policy smoothing이 이를 수행하도록 설계되었다.

Next: **clipped double-Q learning**. 두 개의 Q-function는 하나의 target을 사용한다.
두 Q-function 중 더 작은 target value를 계산한다.

$$y(r, s', d) = r + \gamma (1 - d) \min_{i=1,2} Q_{\phi_{i, targ}}(s', a'(s'))$$

그리고 둘은 이 target에 대해 regressing을 하여 학습한다.

$$L(\phi_1, D) = E_{(s,a,r,s',d) \sim D}[(Q_{\phi_1}(s, a) - y(r, s', d))^2]$$

$$L(\phi_2, D) = E_{(s,a,r,s',d) \sim D}[(Q_{\phi_2}(s, a) - y(r, s', d))^2]$$

target으로 더 작은 Q-value를 사용하여 이를 향해 regress하며, 이는 Q-function에 대한 overestimation을 방지한다.

Lastly: policy는 maximizing $Q_{\phi_1}$로부터 학습한다.

$$\max_{\theta} E_{s \sim D} [Q_{\phi_1}(s, \mu_{\theta}(s)]$$

이는 DDPG에서 크게 바뀌지 않았다. 하지만 TD3에서 policy update는 Q-function 업데이트보다 덜 자주 수행된다.

## Exploration vs. Exploitation

DDPG와 동일

## Pseudocode

```python
# Algorithm 1: Twin Delayed DDPG

# Input: initial policy parameters θ, Q-function parameters φ1, φ2, empty replay buffer D
# Set target parameters equal to main parameters θ_targ <- θ, φ_targ,1 <- φ1, φ_targ,2 <- φ2

while not done:
  # Observe state s and select action a = clip(μ_θ(s) + ε, a_low, a_high), where ε ~ N
  # Execute a in the environment
  # Observe next state s', reward r, and done signal d to indicate whether s' is terminal
  # Store (s, a, r, s', d) in replay buffer D
  # If s' is terminal, reset environment state.
  if time_to_update:
    for i in many_updates:
      # Randomly sample a batch of transitions, B = {(s, a, r, s', d)} from D

      # Compute target actions

      # a'(s') = clip(μθ_targ(s') + clip(ε, -c, c), a_low, a_high))    ε~N(0,σ)

      # Compute target

      # y(r, s', d) = r + γ(1-d) min{i=1,2}Qφ_targ,i(s', a'(s'))

      # Update Q-function by one step of gradient descent using

      # (See Equation in spinning up webpage: https://spinningup.openai.com/en/latest/algorithms/td3.html)

      if i % policy_delay == 0:
        # Update policy by one step of gradient ascent using
        # (See Equation in spinning up webpage: https://spinningup.openai.com/en/latest/algorithms/td3.html)

      # Update target networks

      # φ_targ, i <- ρ φ_targ,i + (1 - ρ) φ      for i = 1, 2
      # θ_targ <- ρ θ_targ + (1 - ρ) θ
```