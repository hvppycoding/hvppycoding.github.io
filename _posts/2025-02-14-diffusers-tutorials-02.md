---
title: "Diffusers - Diffusion ëª¨ë¸ í•™ìŠµí•˜ê¸°"
excerpt: ""
date: 2025-02-14 05:00:00 +0900
classes: wide
header:
  overlay_image: /assets/images/unsplash-emile-perron.jpg
  overlay_filter: 0.5
  caption: "Photo by [**Emile Perron**](https://unsplash.com/@emilep) on [**Unsplash**](https://unsplash.com/)"
categories:
  - Diffusers
---

ì°¸ê³ í•œ ë¬¸ì„œ: [Train a diffusion model](https://huggingface.co/docs/diffusers/tutorials/basic_training)

Unconditional ì´ë¯¸ì§€ ìƒì„±ì€ í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ì…‹ê³¼ ìœ ì‚¬í•œ ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ëŠ” diffusion ëª¨ë¸ì—ì„œ ì¸ê¸° ìˆëŠ” ì–´í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ, ê°€ì¥ ì¢‹ì€ ê²°ê³¼ëŠ” íŠ¹ì • ë°ì´í„°ì…‹ì— ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ê²ƒìœ¼ë¡œ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ í—ˆë¸Œì—ì„œ ì´ëŸ¬í•œ ë§ì€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ìˆì§€ë§Œ, ë§Œì•½ ë§ˆìŒì— ë“œëŠ” ì²´í¬í¬ì¸íŠ¸ë¥¼ ì°¾ì§€ ëª»í–ˆë‹¤ë©´, ì–¸ì œë“ ì§€ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!

ì´ íŠœí† ë¦¬ì–¼ì€ ë‚˜ë§Œì˜ ğŸ¦‹ ë‚˜ë¹„ ğŸ¦‹ë¥¼ ìƒì„±í•˜ê¸° ìœ„í•´ [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) ë°ì´í„°ì…‹ì˜ í•˜ìœ„ ì§‘í•©ì—ì„œ `UNet2DModel` ëª¨ë¸ì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì„ ê°€ë¥´ì³ì¤„ ê²ƒì…ë‹ˆë‹¤.

ğŸ’¡ ì´ í•™ìŠµ íŠœí† ë¦¬ì–¼ì€ [Training with ğŸ§¨ Diffusers](https://colab.research.google.com/github/huggingface/notebooks/blob/main/diffusers/training_example.ipynb) ë…¸íŠ¸ë¶ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤. Diffusion ëª¨ë¸ì˜ ì‘ë™ ë°©ì‹ ë° ìì„¸í•œ ë‚´ìš©ì€ ë…¸íŠ¸ë¶ì„ í™•ì¸í•˜ì„¸ìš”!
{: .notice--info}

ì‹œì‘ ì „ì—, ğŸ¤— Datasetsì„ ë¶ˆëŸ¬ì˜¤ê³  ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ë°ì´í„°ì…‹ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ ë‹¤ìˆ˜ GPUì—ì„œ í•™ìŠµì„ ê°„ì†Œí™”í•˜ê¸° ìœ„í•´ ğŸ¤— Accelerate ê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ê·¸ í›„ í•™ìŠµ ë©”íŠ¸ë¦­ì„ ì‹œê°í™”í•˜ê¸° ìœ„í•´ [TensorBoard](https://www.tensorflow.org/tensorboard)ë¥¼ ì„¤ì¹˜í•˜ì„¸ìš”. (ë˜í•œ í•™ìŠµ ì¶”ì ì„ ìœ„í•´ [Weights & Biases](https://docs.wandb.ai/)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.)

```bash
pip install diffusers[training]
```

ì»¤ë®¤ë‹ˆí‹°ì— ëª¨ë¸ì„ ê³µìœ í•  ê²ƒì„ ê¶Œì¥í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸ì„ í•´ì•¼ í•©ë‹ˆë‹¤. ë…¸íŠ¸ë¶ì—ì„œ í† í°ì„ ì…ë ¥í•˜ì—¬ ë¡œê·¸ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
# ë…¸íŠ¸ë¶
from huggingface_hub import notebook_login

notebook_login()
```

í˜¹ì€ í„°ë¯¸ë„ë¡œ ë¡œê·¸ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```bash
huggingface-cli login
```

ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ê°€ ìƒë‹¹íˆ í¬ê¸° ë•Œë¬¸ì— Git-LFSì—ì„œ ëŒ€ìš©ëŸ‰ íŒŒì¼ì˜ ë²„ì „ ê´€ë¦¬ë¥¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```bash
sudo apt -qq install git-lfs
git config --global credential.helper store
```

## í•™ìŠµ êµ¬ì„±

í¸ì˜ë¥¼ ìœ„í•´ í•™ìŠµ íŒŒë¼ë¯¸í„°ë“¤ì„ í¬í•¨í•œ `TrainingConfig` í´ë˜ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤ (ììœ ë¡­ê²Œ ì¡°ì • ê°€ëŠ¥):

```python
from dataclasses import dataclass


@dataclass
class TrainingConfig:
    image_size = 128  # ìƒì„±ë˜ëŠ” ì´ë¯¸ì§€ í•´ìƒë„
    train_batch_size = 16
    eval_batch_size = 16  # í‰ê°€ ë™ì•ˆì— ìƒ˜í”Œë§í•  ì´ë¯¸ì§€ ìˆ˜
    num_epochs = 50
    gradient_accumulation_steps = 1
    learning_rate = 1e-4
    lr_warmup_steps = 500
    save_image_epochs = 10
    save_model_epochs = 30
    mixed_precision = "fp16"  # `no`ëŠ” float32, ìë™ í˜¼í•© ì •ë°€ë„ë¥¼ ìœ„í•œ `fp16`
    output_dir = "ddpm-butterflies-128"  # ë¡œì»¬ ë° HF Hubì— ì €ì¥ë˜ëŠ” ëª¨ë¸ëª…

    push_to_hub = True  # ì €ì¥ëœ ëª¨ë¸ì„ HF Hubì— ì—…ë¡œë“œí• ì§€ ì—¬ë¶€
    hub_private_repo = None
    overwrite_output_dir = True  # ë…¸íŠ¸ë¶ì„ ë‹¤ì‹œ ì‹¤í–‰í•  ë•Œ ì´ì „ ëª¨ë¸ì— ë®ì–´ì”Œìš¸ì§€
    seed = 0


config = TrainingConfig()
```

## ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°

ğŸ¤— Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì™€ [Smithsonian Butterflies](https://huggingface.co/datasets/huggan/smithsonian_butterflies_subset) ë°ì´í„°ì…‹ì„ ì‰½ê²Œ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from datasets import load_dataset

config.dataset_name = "huggan/smithsonian_butterflies_subset"
dataset = load_dataset(config.dataset_name, split="train")
```

ğŸ’¡[HugGan Community Event](https://huggingface.co/huggan) ì—ì„œ ì¶”ê°€ì˜ ë°ì´í„°ì…‹ì„ ì°¾ê±°ë‚˜ ë¡œì»¬ì˜ [ImageFolder](https://huggingface.co/docs/datasets/image_dataset#imagefolder)ë¥¼ ë§Œë“¦ìœ¼ë¡œì¨ ë‚˜ë§Œì˜ ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. HugGan Community Event ì— ê°€ì ¸ì˜¨ ë°ì´í„°ì…‹ì˜ ê²½ìš° ë¦¬í¬ì§€í† ë¦¬ì˜ idë¡œ `config.dataset_name` ì„ ì„¤ì •í•˜ê³ , ë‚˜ë§Œì˜ ì´ë¯¸ì§€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²½ìš° `imagefolder` ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

ğŸ¤— Datasetsì€ `Image` ê¸°ëŠ¥ì„ ì‚¬ìš©í•´ ìë™ìœ¼ë¡œ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë””ì½”ë”©í•˜ê³  [`PIL.Image`](https://pillow.readthedocs.io/en/stable/reference/Image.html)ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. ì´ë¥¼ ì‹œê°í™” í•´ë³´ë©´:

```python
import matplotlib.pyplot as plt

fig, axs = plt.subplots(1, 4, figsize=(16, 4))
for i, image in enumerate(dataset[:4]["image"]):
    axs[i].imshow(image)
    axs[i].set_axis_off()
fig.show()
```

ì´ë¯¸ì§€ëŠ” ëª¨ë‘ ë‹¤ë¥¸ ì‚¬ì´ì¦ˆì´ê¸° ë•Œë¬¸ì—, ìš°ì„  ì „ì²˜ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤:

- `Resize`ëŠ” `config.image_size`ì— ì •ì˜ëœ ì´ë¯¸ì§€ ì‚¬ì´ì¦ˆë¡œ ë³€ê²½í•©ë‹ˆë‹¤.
- `RandomHorizontalFlip`ì€ ëœë¤ì ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ Flipí•˜ì—¬ ë°ì´í„°ì…‹ì„ ë³´ê°•í•©ë‹ˆë‹¤.
- `Normalize`ëŠ” ëª¨ë¸ì´ ì˜ˆìƒí•˜ëŠ” `[-1, 1]` ë²”ìœ„ë¡œ í”½ì…€ ê°’ì„ ì¬ì¡°ì •í•©ë‹ˆë‹¤.

```python
from torchvision import transforms

preprocess = transforms.Compose(
    [
        transforms.Resize((config.image_size, config.image_size)),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize([0.5], [0.5]),
    ]
)
```

í•™ìŠµ ë„ì¤‘ì— `preprocess` í•¨ìˆ˜ë¥¼ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasetsì˜ `set_transform` ë°©ë²•ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.

```python
def transform(examples):
    images = [preprocess(image.convert("RGB")) for image in examples["image"]]
    return {"images": images}


dataset.set_transform(transform)
```

ì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ì¡°ì •ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ë¥¼ ë‹¤ì‹œ ì‹œê°í™”í•´ë³´ì„¸ìš”. ì´ì œ [DataLoader](https://pytorch.org/docs/stable/data#torch.utils.data.DataLoader)ì— ë°ì´í„°ì…‹ì„ í¬í•¨í•´ í•™ìŠµí•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!

## UNet2DModel ìƒì„±í•˜ê¸°

ğŸ§¨ Diffusersì— ì‚¬ì „í•™ìŠµëœ ëª¨ë¸ë“¤ì€ ëª¨ë¸ í´ë˜ìŠ¤ì—ì„œ ì›í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ ì‰½ê²Œ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, UNet2DModelë¥¼ ìƒì„±í•˜ë ¤ë©´:

```python
from diffusers import UNet2DModel

model = UNet2DModel(
    sample_size=config.image_size,  # íƒ€ê²Ÿ ì´ë¯¸ì§€ í•´ìƒë„
    in_channels=3,  # ì…ë ¥ ì±„ë„ ìˆ˜, RGB ì´ë¯¸ì§€ì—ì„œ 3
    out_channels=3,  # ì¶œë ¥ ì±„ë„ ìˆ˜
    layers_per_block=2,  # UNet ë¸”ëŸ­ë‹¹ ëª‡ ê°œì˜ ResNet ë ˆì´ì–´ê°€ ì‚¬ìš©ë˜ëŠ”ì§€
    block_out_channels=(128, 128, 256, 256, 512, 512),  # ê° UNet ë¸”ëŸ­ì„ ìœ„í•œ ì¶œë ¥ ì±„ë„ ìˆ˜
    down_block_types=(
        "DownBlock2D",  # ì¼ë°˜ì ì¸ ResNet ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ëŸ­
        "DownBlock2D",
        "DownBlock2D",
        "DownBlock2D",
        "AttnDownBlock2D",  # spatial self-attentionì´ í¬í•¨ëœ ì¼ë°˜ì ì¸ ResNet ë‹¤ìš´ìƒ˜í”Œë§ ë¸”ëŸ­
        "DownBlock2D",
    ),
    up_block_types=(
        "UpBlock2D",  # ì¼ë°˜ì ì¸ ResNet ì—…ìƒ˜í”Œë§ ë¸”ëŸ­
        "AttnUpBlock2D",  # spatial self-attentionì´ í¬í•¨ëœ ì¼ë°˜ì ì¸ ResNet ì—…ìƒ˜í”Œë§ ë¸”ëŸ­
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
        "UpBlock2D",
    ),
)
```

ìƒ˜í”Œì˜ ì´ë¯¸ì§€ í¬ê¸°ì™€ ëª¨ë¸ ì¶œë ¥ í¬ê¸°ê°€ ë§ëŠ”ì§€ ë¹ ë¥´ê²Œ í™•ì¸í•˜ê¸° ìœ„í•œ ì¢‹ì€ ì•„ì´ë””ì–´ê°€ ìˆìŠµë‹ˆë‹¤:

```python
sample_image = dataset[0]["images"].unsqueeze(0)
print("Input shape:", sample_image.shape)
# Input shape: torch.Size([1, 3, 128, 128])
print("Output shape:", model(sample_image, timestep=0).sample.shape)
# Output shape: torch.Size([1, 3, 128, 128])
```

í›Œë¥­í•´ìš”! ë‹¤ìŒ, ì´ë¯¸ì§€ì— ì•½ê°„ì˜ ë…¸ì´ì¦ˆë¥¼ ë”í•˜ê¸° ìœ„í•´ ìŠ¤ì¼€ì¤„ëŸ¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.

## ìŠ¤ì¼€ì¤„ëŸ¬ ìƒì„±í•˜ê¸°

ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ëª¨ë¸ì„ í•™ìŠµ ë˜ëŠ” ì¶”ë¡ ì— ì‚¬ìš©í•˜ëŠ”ì§€ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì‘ë™í•©ë‹ˆë‹¤. ì¶”ë¡ ì‹œì—, ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” ë…¸ì´ì¦ˆë¡œë¶€í„° ì´ë¯¸ì§€ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. í•™ìŠµì‹œ ìŠ¤ì¼€ì¤„ëŸ¬ëŠ” diffusion ê³¼ì •ì—ì„œì˜ íŠ¹ì • í¬ì¸íŠ¸ë¡œë¶€í„° ëª¨ë¸ì˜ ì¶œë ¥ ë˜ëŠ” ìƒ˜í”Œì„ ê°€ì ¸ì™€ ë…¸ì´ì¦ˆ ìŠ¤ì¼€ì¤„ ê³¼ ì—…ë°ì´íŠ¸ ê·œì¹™ì— ë”°ë¼ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì ìš©í•©ë‹ˆë‹¤.

`DDPMScheduler`ë¥¼ ë³´ë©´ ì´ì „ìœ¼ë¡œë¶€í„° `sample_image`ì— ëœë¤í•œ ë…¸ì´ì¦ˆë¥¼ ë”í•˜ëŠ” `add_noise` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤:

```python
import torch
from PIL import Image
from diffusers import DDPMScheduler

noise_scheduler = DDPMScheduler(num_train_timesteps=1000)
noise = torch.randn(sample_image.shape)
timesteps = torch.LongTensor([50])
noisy_image = noise_scheduler.add_noise(sample_image, noise, timesteps)

Image.fromarray(((noisy_image.permute(0, 2, 3, 1) + 1.0) * 127.5).type(torch.uint8).numpy()[0])
```

ëª¨ë¸ì˜ í•™ìŠµ ëª©ì ì€ ì´ë¯¸ì§€ì— ë”í•´ì§„ ë…¸ì´ì¦ˆë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë‹¨ê³„ì—ì„œ ì†ì‹¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
import torch.nn.functional as F

noise_pred = model(noisy_image, timesteps).sample
loss = F.mse_loss(noise_pred, noise)
```

## ëª¨ë¸ í•™ìŠµí•˜ê¸°

ì§€ê¸ˆê¹Œì§€, ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•˜ê¸° ìœ„í•´ ë§ì€ ë¶€ë¶„ì„ ê°–ì¶”ì—ˆìœ¼ë©° ì´ì œ ë‚¨ì€ ê²ƒì€ ëª¨ë“  ê²ƒì„ ì¡°í•©í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

ìš°ì„  ì˜µí‹°ë§ˆì´ì €(optimizer)ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler)ê°€ í•„ìš”í•  ê²ƒì…ë‹ˆë‹¤:

```python
from diffusers.optimization import get_cosine_schedule_with_warmup

optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
lr_scheduler = get_cosine_schedule_with_warmup(
    optimizer=optimizer,
    num_warmup_steps=config.lr_warmup_steps,
    num_training_steps=(len(train_dataloader) * config.num_epochs),
)
```

ê·¸ í›„, ëª¨ë¸ì„ í‰ê°€í•˜ëŠ” ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. í‰ê°€ë¥¼ ìœ„í•´, `DDPMPipeline`ì„ ì‚¬ìš©í•´ ë°°ì¹˜ì˜ ì´ë¯¸ì§€ ìƒ˜í”Œë“¤ì„ ìƒì„±í•˜ê³  ê·¸ë¦¬ë“œ í˜•íƒœë¡œ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
from diffusers import DDPMPipeline
import math
import os


def make_grid(images, rows, cols):
    w, h = images[0].size
    grid = Image.new("RGB", size=(cols * w, rows * h))
    for i, image in enumerate(images):
        grid.paste(image, box=(i % cols * w, i // cols * h))
    return grid


def evaluate(config, epoch, pipeline):
    # ëœë¤í•œ ë…¸ì´ì¦ˆë¡œ ë¶€í„° ì´ë¯¸ì§€ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.(ì´ëŠ” ì—­ì „íŒŒ diffusion ê³¼ì •ì…ë‹ˆë‹¤.)
    # ê¸°ë³¸ íŒŒì´í”„ë¼ì¸ ì¶œë ¥ í˜•íƒœëŠ” `List[PIL.Image]` ì…ë‹ˆë‹¤.
    images = pipeline(
        batch_size=config.eval_batch_size,
        generator=torch.manual_seed(config.seed),
    ).images

    # ì´ë¯¸ì§€ë“¤ì„ ê·¸ë¦¬ë“œë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤.
    image_grid = make_grid(images, rows=4, cols=4)

    # ì´ë¯¸ì§€ë“¤ì„ ì €ì¥í•©ë‹ˆë‹¤.
    test_dir = os.path.join(config.output_dir, "samples")
    os.makedirs(test_dir, exist_ok=True)
    image_grid.save(f"{test_dir}/{epoch:04d}.png")
```

TensorBoardì— ë¡œê¹…, ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì  ë° í˜¼í•© ì •ë°€ë„ í•™ìŠµì„ ì‰½ê²Œ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ ğŸ¤— Accelerateë¥¼ í•™ìŠµ ë£¨í”„ì— í•¨ê»˜ ì•ì„œ ë§í•œ ëª¨ë“  êµ¬ì„± ì •ë³´ë“¤ì„ ë¬¶ì–´ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í—ˆë¸Œì— ëª¨ë¸ì„ ì—…ë¡œë“œ í•˜ê¸° ìœ„í•´ ë¦¬í¬ì§€í† ë¦¬ ì´ë¦„ ë° ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ê¸° ìœ„í•œ í•¨ìˆ˜ë¥¼ ì‘ì„±í•˜ê³  í—ˆë¸Œì— ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ğŸ’¡ì•„ë˜ì˜ í•™ìŠµ ë£¨í”„ëŠ” ì–´ë µê³  ê¸¸ì–´ ë³´ì¼ ìˆ˜ ìˆì§€ë§Œ, ë‚˜ì¤‘ì— í•œ ì¤„ì˜ ì½”ë“œë¡œ í•™ìŠµì„ í•œë‹¤ë©´ ê·¸ë§Œí•œ ê°€ì¹˜ê°€ ìˆì„ ê²ƒì…ë‹ˆë‹¤! ë§Œì•½ ê¸°ë‹¤ë¦¬ì§€ ëª»í•˜ê³  ì´ë¯¸ì§€ë¥¼ ìƒì„±í•˜ê³  ì‹¶ë‹¤ë©´, ì•„ë˜ ì½”ë“œë¥¼ ììœ ë¡­ê²Œ ë¶™ì—¬ë„£ê³  ì‘ë™ì‹œí‚¤ë©´ ë©ë‹ˆë‹¤. ğŸ¤—

```python
from accelerate import Accelerator
from huggingface_hub import create_repo, upload_folder
from tqdm.auto import tqdm
from pathlib import Path
import os


def train_loop(config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler):
    # Initialize accelerator and tensorboard logging
    accelerator = Accelerator(
        mixed_precision=config.mixed_precision,
        gradient_accumulation_steps=config.gradient_accumulation_steps,
        log_with="tensorboard",
        project_dir=os.path.join(config.output_dir, "logs"),
    )
    if accelerator.is_main_process:
        if config.output_dir is not None:
            os.makedirs(config.output_dir, exist_ok=True)
        if config.push_to_hub:
            repo_id = create_repo(
                repo_id=config.hub_model_id or Path(config.output_dir).name, exist_ok=True
            ).repo_id
        accelerator.init_trackers("train_example")

    # ëª¨ë“  ê²ƒì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.
    # ê¸°ì–µí•´ì•¼ í•  íŠ¹ì •í•œ ìˆœì„œëŠ” ì—†ìœ¼ë©° ì¤€ë¹„í•œ ë°©ë²•ì— ì œê³µí•œ ê²ƒê³¼ ë™ì¼í•œ ìˆœì„œë¡œ ê°ì²´ì˜ ì••ì¶•ì„ í’€ë©´ ë©ë‹ˆë‹¤.
    model, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(
        model, optimizer, train_dataloader, lr_scheduler
    )

    global_step = 0

    # ì´ì œ ëª¨ë¸ì„ í•™ìŠµí•©ë‹ˆë‹¤.
    for epoch in range(config.num_epochs):
        progress_bar = tqdm(total=len(train_dataloader), disable=not accelerator.is_local_main_process)
        progress_bar.set_description(f"Epoch {epoch}")

        for step, batch in enumerate(train_dataloader):
            clean_images = batch["images"]
            # ì´ë¯¸ì§€ì— ë”í•  ë…¸ì´ì¦ˆë¥¼ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
            noise = torch.randn(clean_images.shape, device=clean_images.device)
            bs = clean_images.shape[0]

            # ê° ì´ë¯¸ì§€ë¥¼ ìœ„í•œ ëœë¤í•œ íƒ€ì„ìŠ¤í…(timestep)ì„ ìƒ˜í”Œë§í•©ë‹ˆë‹¤.
            timesteps = torch.randint(
                0, noise_scheduler.config.num_train_timesteps, (bs,), device=clean_images.device,
                dtype=torch.int64
            )

            # ê° íƒ€ì„ìŠ¤í…ì˜ ë…¸ì´ì¦ˆ í¬ê¸°ì— ë”°ë¼ ê¹¨ë—í•œ ì´ë¯¸ì§€ì— ë…¸ì´ì¦ˆë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.
            # (ì´ëŠ” foward diffusion ê³¼ì •ì…ë‹ˆë‹¤.)
            noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)

            with accelerator.accumulate(model):
                # ë…¸ì´ì¦ˆë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì˜ˆì¸¡í•©ë‹ˆë‹¤.
                noise_pred = model(noisy_images, timesteps, return_dict=False)[0]
                loss = F.mse_loss(noise_pred, noise)
                accelerator.backward(loss)

                accelerator.clip_grad_norm_(model.parameters(), 1.0)
                optimizer.step()
                lr_scheduler.step()
                optimizer.zero_grad()

            progress_bar.update(1)
            logs = {"loss": loss.detach().item(), "lr": lr_scheduler.get_last_lr()[0], "step": global_step}
            progress_bar.set_postfix(**logs)
            accelerator.log(logs, step=global_step)
            global_step += 1

        # ê° ì—í¬í¬ê°€ ëë‚œ í›„ evaluate()ì™€ ëª‡ ê°€ì§€ ë°ëª¨ ì´ë¯¸ì§€ë¥¼ ì„ íƒì ìœ¼ë¡œ ìƒ˜í”Œë§í•˜ê³  ëª¨ë¸ì„ ì €ì¥í•©ë‹ˆë‹¤.
        if accelerator.is_main_process:
            pipeline = DDPMPipeline(unet=accelerator.unwrap_model(model), scheduler=noise_scheduler)

            if (epoch + 1) % config.save_image_epochs == 0 or epoch == config.num_epochs - 1:
                evaluate(config, epoch, pipeline)

            if (epoch + 1) % config.save_model_epochs == 0 or epoch == config.num_epochs - 1:
                if config.push_to_hub:
                    upload_folder(
                        repo_id=repo_id,
                        folder_path=config.output_dir,
                        commit_message=f"Epoch {epoch}",
                        ignore_patterns=["step_*", "epoch_*"],
                    )
                else:
                    pipeline.save_pretrained(config.output_dir)
```

íœ´, ì½”ë“œê°€ ê½¤ ë§ì•˜ë„¤ìš”! í•˜ì§€ë§Œ ğŸ¤— Accelerateì˜ `notebook_launcher` í•¨ìˆ˜ì™€ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. í•¨ìˆ˜ì— í•™ìŠµ ë£¨í”„, ëª¨ë“  í•™ìŠµ ì¸ìˆ˜, í•™ìŠµì— ì‚¬ìš©í•  í”„ë¡œì„¸ìŠ¤ ìˆ˜(ì‚¬ìš© ê°€ëŠ¥í•œ GPUì˜ ìˆ˜ë¥¼ ë³€ê²½í•  ìˆ˜ ìˆìŒ)ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤:

```python
from accelerate import notebook_launcher

args = (config, model, noise_scheduler, optimizer, train_dataloader, lr_scheduler)

notebook_launcher(train_loop, args, num_processes=1)
```

í•œë²ˆ í•™ìŠµì´ ì™„ë£Œë˜ë©´, diffusion ëª¨ë¸ë¡œ ìƒì„±ëœ ìµœì¢… ğŸ¦‹ì´ë¯¸ì§€ğŸ¦‹ë¥¼ í™•ì¸í•´ë³´ê¸¸ ë°”ëë‹ˆë‹¤!

```python
import glob

sample_images = sorted(glob.glob(f"{config.output_dir}/samples/*.png"))
Image.open(sample_images[-1])
```

![outputs]({{site.baseurl}}/assets/images/2025-02-14-diffusers-tutorials-02/outputs.png){: .align-center}  
